{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"YoloV4_Real_Time_test.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPEqvqS2gXR1+fTn5jw/cig"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"mGcF_q1thqeS"},"source":["# Importing Libraries\n","import numpy as np\n","import pandas as pd\n","import cv2\n","import time\n","from timeit import default_timer as timer\n","import matplotlib.pyplot as plt\n","import pickle\n","from tensorflow.keras.models import load_model\n","import tensorflow as tf\n","import os"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AoJVaFAsh0ST"},"source":["# CNN Classes\n","signs = pd.read_csv('yolo-coco-data/label_names.csv')\n","\n","# YOLO Classes\n","with open('yolo-coco-data/coco.names') as f:\n","    labels = [line.strip() for line in f]\n","\n","# CNN Model\n","model = load_model('yolo-coco-data/model.h5')\n","\n","# Pathes to CFG file and Weights\n","path_to_weights = 'yolo-coco-data/yolo.weights'\n","path_to_cfg = 'yolo-coco-data/yolo.cfg'\n","\n","# Load YOLO\n","network = cv2.dnn.readNetFromDarknet(path_to_cfg, path_to_weights)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ITO3fmm8ifzB"},"source":["# To use with GPU\n","network.setPreferableBackend(cv2.dnn.DNN_BACKEND_OPENCV)\n","network.setPreferableTarget(cv2.dnn.DNN_TARGET_OPENCL_FP16)\n","\n","# Getting names of all YOLO layers\n","layers_all = network.getLayerNames()\n","\n","# Getting Unconnected Layers YOLO\n","layers_names_output = [layers_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aNtUa43uisUz"},"source":["# Minimum Probability to eliminate weak predictions\n","probability_minimum = 0.2\n","\n","# Threshold for filtering weak bounding boxes with maximum supression\n","threshold = 0.2\n","\n","# Random Colors for each yolo Class\n","colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7c8R_nnki2KQ"},"source":["# Real Time Video Stream\n","camera = cv2.VideoCapture(0)\n","\n","# Variables for frame dimensions\n","h, w = None, None\n","\n","# Writer for saving video\n","writer = None\n","\n","# Setting default size of plots\n","plt.rcParams['figure.figsize'] = (3, 3)\n","\n","# Frame Count\n","f = 0\n","\n","# Time spent\n","t = 0\n","\n","# Setting default size of plots\n","plt.rcParams['figure.figsize'] = (3, 3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bt4SshyYjUPy"},"source":["# Function for Grayscaling the image\n","def grayscale(img):\n","    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n","    return img\n","\n","# Function for equalizing image using Histogram Equalizer\n","def equalize(img):\n","    img = cv2.equalizeHist(img)\n","    return img\n","\n","# Preprocessing function that uses Grayscaling, Equalizing and Normalizing by division to 255\n","def preprocessing(img):\n","    img = grayscale(img)\n","    img = equalize(img)\n","    img = img / 255\n","    return img"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F-BZ8E3rjWfa"},"source":["# Catching frames in the loop\n","while True:\n","    # Capturing frames one-by-one\n","    ret, frame = camera.read()\n","\n","    # If the frame was not retrieved\n","    if not ret:\n","        break\n","\n","    # Getting spatial dimensions of the frame for the first time\n","    if w is None or h is None:\n","        # Slicing two elements from tuple\n","        h, w = frame.shape[:2]\n","\n","    # Blob from current frame\n","    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416), swapRB=True, crop=False)\n","\n","    # Forward pass with blob through output layers\n","    network.setInput(blob)\n","    start = time.time()\n","    output_from_network = network.forward(layers_names_output)\n","    end = time.time()\n","\n","    # Increasing counters\n","    f += 1\n","    t += end - start\n","\n","    # Spent time for current frame\n","    print('Frame number {0} took {1:.5f} seconds'.format(f, end - start))\n","\n","    # Lists for detected bounding boxes, confidences and class's number\n","    bounding_boxes = []\n","    confidences = []\n","    class_numbers = []\n","\n","    # Going through all output layers after feed forward pass\n","    for result in output_from_network:\n","        # Going through all detections from current output layer\n","        for detected_objects in result:\n","            # Getting 80 classes' probabilities for current detected object\n","            scores = detected_objects[5:]\n","            # Getting index of the class with the maximum value of probability\n","            class_current = np.argmax(scores)\n","            # Getting value of probability for defined class\n","            confidence_current = scores[class_current]\n","\n","            # Eliminating weak predictions by minimum probability\n","            if confidence_current > probability_minimum:\n","                # Scaling bounding box coordinates to the initial frame size\n","                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n","\n","                # Getting top left corner coordinates\n","                x_center, y_center, box_width, box_height = box_current\n","                x_min = int(x_center - (box_width / 2))\n","                y_min = int(y_center - (box_height / 2))\n","\n","                # Adding results into prepared lists\n","                bounding_boxes.append([x_min, y_min, int(box_width), int(box_height)])\n","                confidences.append(float(confidence_current))\n","                class_numbers.append(class_current)\n","\n","    # Implementing non-maximum suppression of given bounding boxes\n","    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences, probability_minimum, threshold)\n","\n","    # Checking if there is any detected object been left\n","    if len(results) > 0:\n","        # Going through indexes of results\n","        for i in results.flatten():\n","            # Bounding box coordinates, its width and height\n","            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n","            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n","\n","            # If traffic sign is detected\n","            if(class_numbers[i] == 8):\n","                # Cut fragment with Traffic Sign\n","                c_ts = frame[y_min:y_min + int(box_height), x_min:x_min + int(box_width), :]\n","\n","                if c_ts.shape[:1] == (0,) or c_ts.shape[1:2] == (0,):\n","                    pass\n","                else:\n","                    # Preprocessing Frame\n","                    img = np.asarray(c_ts)\n","                    img = cv2.resize(img, (32, 32))\n","                    img = preprocessing(img)\n","                    img = img.reshape(1, 32, 32, 1)\n","\n","                    # Feeding to the Keras CNN model to get predicted label among 81 classes\n","                    scores = model.predict(img)\n","\n","                    # Scores is given for image with 81 numbers of predictions for each class\n","                    # Getting only one class with maximum value\n","                    prediction = np.argmax(scores)\n","\n","                    # Colour for current bounding box\n","                    colour_box_current = colours[class_numbers[i]].tolist()\n","\n","                    # Drawing bounding box on the original current frame\n","                    cv2.rectangle(frame, (x_min, y_min),\n","                                  (x_min + box_width, y_min + box_height),\n","                                  colour_box_current, 2)\n","\n","                    # Preparing text with label and confidence for current bounding box\n","                    text_box_current = '{}: {:.4f}'.format(signs['SignName'][prediction],\n","                                                           confidences[i])\n","\n","                    # Putting text with label and confidence on the original image\n","                    cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n","                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n","            else:\n","                # Preparing colour for current bounding box\n","                # and converting from numpy array to list\n","                colour_box_current = colours[class_numbers[i]].tolist()\n","\n","                # Drawing bounding box on the original current frame\n","                cv2.rectangle(frame, (x_min, y_min),\n","                              (x_min + box_width, y_min + box_height),\n","                              colour_box_current, 2)\n","\n","                # Preparing text with label and confidence for current bounding box\n","                text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n","                                                       confidences[i])\n","\n","                # Putting text with label and confidence on the original image\n","                cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n","                            cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n","\n","    # Showing results obtained from camera in Real Time\n","    cv2.namedWindow('YOLO Real Time Detections', cv2.WINDOW_NORMAL)\n","    cv2.imshow('YOLO Real Time Detections', frame)\n","\n","    # Breaking the loop if 'q' is pressed\n","    if cv2.waitKey(1) & 0xFF == ord('q'):\n","        break"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z-6sfHD9kLo1"},"source":["# Releasing video reader and writer\n","camera.release()\n","cv2.destroyAllWindows()"],"execution_count":null,"outputs":[]}]}